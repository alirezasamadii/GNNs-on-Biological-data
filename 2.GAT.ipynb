{"cells":[{"cell_type":"markdown","metadata":{"id":"UMdQnPhTPRpa"},"source":["good sources about this model :\n","\n","https://youtu.be/A-yKQamf2Fc\n","\n","https://dsgiitr.com/blogs/gat/"]},{"cell_type":"markdown","metadata":{"id":"5VHE2oJqyjPM"},"source":["Can we do better than GCNs? \n","\n","From Graph Convolutional Network (GCN), we learnt that combining local graph structure and node-level features yields good performance on node classification task. However, the way GCN aggregates messages is structure-dependent, which may hurt its generalizability.\n","\n","\n","**GAT (Graph Attention Network)**, is a novel neural network architecture that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.\n","The fundamental novelty that GAT brings to the table is how the information from the one-hop neighborhood is aggregated."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25676,"status":"ok","timestamp":1645087997137,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"},"user_tz":-60},"id":"LaxKDht-PNcn","outputId":"9d377d3d-25a5-4797-8872-8c00561fe967"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks/DMFOLDER/DATA\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/Colab Notebooks/DMFOLDER/DATA"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"SvpXQKPAXjoW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645088027011,"user_tz":-60,"elapsed":29898,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}},"outputId":"8ca564af-b282-40dd-c740-32b21e695218"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 7.9 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 3.5 MB 5.4 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 5.4 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 44.0 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 2.6 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 49.0 MB/s \n","\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install required packages of pytorch geometric\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","#then import needed ones\n","import pandas as pd\n","import sys\n","from tqdm import tqdm, tqdm_notebook\n","import pandas as pd\n","import networkx as nx\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import os.path as osp\n","import matplotlib.pyplot as plt\n","import time\n","import warnings\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GATConv\n","from torch_geometric.datasets import Planetoid\n","import torch_geometric.transforms as T\n","\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, GNNExplainer\n","\n","from sklearn.model_selection import train_test_split\n","from torch_geometric.data import InMemoryDataset\n","from sklearn.preprocessing import StandardScaler\n","from torch_geometric.utils import metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8TBDGuHXsCO"},"outputs":[],"source":["def filter_disease(disease_gene_association_file):\n","    columns = ['diseaseName', 'geneSymbol']\n","    df = pd.read_csv(disease_gene_association_file, sep='\\t',usecols=columns)\n","    \n","    is_disease =  df['diseaseName']==\"Schizophrenia\" # first disease to be examined , comment this for checking for other disease and uncooment following\n","    #is_disease =  df['diseaseName']==\"Asthma\"\n","    df_disease = df[is_disease]\n","    return df_disease"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aj7wvOW0Xsju"},"outputs":[],"source":["def create_G(biogrid_file_name,df_disease):\n","    biogrid = open(biogrid_file_name, 'r')\n","    lines=biogrid.readlines()\n","    G=nx.Graph()\n","    for line in tqdm(lines):\n","        x=line.split()\n","        if df_disease['geneSymbol'].str.contains(x[0]).any():\n","            G.add_node(x[0],label=1)\n","        else:\n","            G.add_node(x[0],label=0)\n","        if df_disease['geneSymbol'].str.contains(x[1]).any(): \n","            G.add_node(x[1],label=1)\n","        else:  \n","            G.add_node(x[1],label=0)\n","        G.add_edge(x[0], x[1])\n","      #  G.add_edge(x[0], x[0]) # i decided to include an edge from node to itself to have adj matrix with himself included\n","       # G.add_edge(x[1], x[1])\n","    biogrid.close()\n","    G=nx.convert_node_labels_to_integers(G)\n","    #print(nx.info(G))\n","    return G"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45SOaMbXGpz_"},"outputs":[],"source":["def create_balanced_G(biogrid_file_name,df_disease):\n","    biogrid = open(biogrid_file_name, 'r')\n","    lines=biogrid.readlines()\n","    G=nx.Graph()\n","    ones=zeros=0\n","    i=0\n","    nums=[0]\n","    for line in tqdm(lines):\n","        x=line.split()\n","        if df_disease['geneSymbol'].str.contains(x[0]).any():\n","            G.add_node(x[0],label=1)\n","            ones=1+ones\n","        else :\n","          num=random.randint(0,100)\n","          if num in nums:\n","             G.add_node(x[0],label=0)\n","             zeros=zeros+1\n","             \n","\n","        if df_disease['geneSymbol'].str.contains(x[1]).any(): \n","            G.add_node(x[1],label=1)\n","            ones=1+ones\n","        else:\n","          num=random.randint(0,4000)\n","          if num in nums:\n","            G.add_node(x[1],label=0)\n","            zeros=1+zeros\n","        \n","        if G.has_node(x[0]) and G.has_node(x[1]):\n","            G.add_edge(x[0], x[1])\n","        i+=1\n","        if i%100000 ==0:\n","            #print(\"p = \",ones/(ones+zeros))\n","            #print(ones)\n","            #print(zeros)\n","            attribute_counter(G)\n","        \n","\n","    biogrid.close()\n","    G=nx.convert_node_labels_to_integers(G)\n","    #print(nx.info(G))\n","    return G"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_qbzfL1jGrw7","executionInfo":{"status":"ok","timestamp":1645088038632,"user_tz":-60,"elapsed":209,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["def attribute_counter(G):\n","  zeros=0\n","  ones=0\n","  for node in tqdm(list(G.nodes())):\n","    if G.nodes[node]['label']==0:\n","      zeros+=1\n","    else:\n","      ones+=1\n","  print(\"#zeros: \", zeros)\n","  print(\"#ones: \", ones)\n","  print(\"portion of ones \" ,ones/(ones+zeros) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JL4HllEuXu2-"},"outputs":[],"source":["print(\" (0/6) started \")\n","#disease_gene_association_file=\"all_gene_disease_associations.tsv\"  #current directory / this is real file not test file\n","disease_gene_association_file=\"curated_gene_disease_associations.tsv\"\n","df_disease=filter_disease(disease_gene_association_file)\n","print(\" (1/6) disease dataframe creation : done\")\n","                   \n","#biogrid_file_name='Biogrid_REDUX.txt'\n","biogrid_file_name='Biogrid_4.4.199_Hs.txt'\n","G=create_G(biogrid_file_name,df_disease)\n","print(\" (2/6) Graph creation : done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLRfirshha_M"},"outputs":[],"source":["nx.write_gpickle(G, \"Asthma_main.gpickle\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWMq7FLChdVH"},"outputs":[],"source":["G = nx.read_gpickle(\"Asthma_main.gpickle\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JX6oHN-zhfjF"},"outputs":[],"source":["print(nx.info(G))\n","print(G.nodes)\n","attribute_counter(G)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n_FP_xchjx9"},"outputs":[],"source":["balanced_G = create_balanced_G(biogrid_file_name,df_disease)"]},{"cell_type":"code","source":["attribute_counter(balanced_G)"],"metadata":{"id":"6GJ1kPdMbJxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2EubmxhhnCE"},"outputs":[],"source":["nx.write_gpickle(balanced_G, \"Asthma_balanced_more.gpickle\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"hZKkcunWhpY-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645088045771,"user_tz":-60,"elapsed":484,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}},"outputId":"df8d1983-6d7f-4f20-ab4c-bb8c870076bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5861/5861 [00:00<00:00, 113744.03it/s]"]},{"output_type":"stream","name":"stdout","text":["#zeros:  3681\n","#ones:  2180\n","portion of ones  0.37195017915031564\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["balanced_G = nx.read_gpickle(\"Asthma_balanced_more.gpickle\")\n","nx.info(balanced_G)\n","attribute_counter(balanced_G)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KigdPrvLhuP1","executionInfo":{"status":"ok","timestamp":1645088051530,"user_tz":-60,"elapsed":711,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["G=balanced_G"]},{"cell_type":"markdown","source":["# Up to here everything is Graph prepraration and from now on is preparing data to be fed into NN. so instead of running all previous (time consuming steps , I load previously pickle saved models of graphs , for two diseases, for balanced and main version of graph derived from all_gene_disease_association ( or the other dataset)"],"metadata":{"id":"R-gWSNzO23dk"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"pD2mwAQ9XyTW","executionInfo":{"status":"ok","timestamp":1645088055797,"user_tz":-60,"elapsed":1992,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["# retrieve the labels for each node, the nodes that are not zero\n","labels = np.asarray([G.nodes[i]['label'] != 0 for i in G.nodes]).astype(np.int64)\n","\n","# create edge index. We need to have data as previously shown. We can exploit networkX and scipy for that \n","adj = nx.to_scipy_sparse_matrix(G).tocoo() #coordinate format\n","#print(adj)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"urvd2BYrX7ef","executionInfo":{"status":"ok","timestamp":1645088055799,"user_tz":-60,"elapsed":23,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["#create edge index in the proper way\n","row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long) #create a torch tensor from numpy in long format : for row indexes\n","col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long) #                                                   for column indexes\n","edge_index = torch.stack([row, col], dim=0)\n","#display(edge_index)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1645088057857,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"},"user_tz":-60},"id":"hePeVDR-BBTR","outputId":"21a6976b-4129-49f4-be45-f332b2042b2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.08454254]\n"," [-0.31812753]\n"," [-0.23271266]\n"," ...\n"," [-0.06188294]\n"," [-0.61097848]\n"," [ 1.79283978]]\n"]}],"source":["# using degree as embedding. For simplicity, the feature vector describing the \n","# will be just its degree, which is enough for us   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.. can i use other embeddings? https://medium.com/@st3llasia/graph-embedding-techniques-7d5386c88c5\n","#is it actually working or we do it for simplicity\n","embeddings = np.array(list(dict(G.degree()).values())) #list the values of degree of each node as numpy array\n","# normalizing degree values\n","\n","scale = StandardScaler()\n","embeddings = scale.fit_transform(embeddings.reshape(-1,1))\n","print(embeddings)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"IY39RikOBEmZ","executionInfo":{"status":"ok","timestamp":1645088059269,"user_tz":-60,"elapsed":5,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["# custom pytorch dataset\n","class PPIDATASET(InMemoryDataset):\n","    def __init__(self, transform=None):\n","        super(PPIDATASET, self).__init__('.', transform, None, None) #pre transform and pre filter: None, we don't need them\n","        data = Data(edge_index=edge_index) #Data : A data object describing a homogeneous graph.  for more : https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data\n","        data.num_nodes = G.number_of_nodes()\n","        # embedding \n","        data.x = torch.from_numpy(embeddings).type(torch.float32)\n","        # labels\n","        y = torch.from_numpy(labels).type(torch.long)\n","        data.y = y.clone().detach() #removing tensors computational graph for efficency since it is not needed\n","        data.num_classes = 2\n","        # splitting the data into train, validation and test\n","        X_train, X_test, y_train, y_test = train_test_split(pd.Series(G.nodes()),  pd.Series(labels), test_size=0.30, random_state=42)\n","        n_nodes = G.number_of_nodes()\n","        # create train and test masks for data\n","        # the Data objects holds a label for each node, and additional node-level attributes: train_mask, val_mask and test_mask, where\n","        #train_mask denotes against which nodes to train (140 nodes),\n","        #val_mask denotes which nodes to use for validation, e.g., t\n","        #test_mask denotes against which nodes to test \n","        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n","        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n","        train_mask[X_train.index] = True\n","        test_mask[X_test.index] = True\n","        data['train_mask'] = train_mask\n","        data['test_mask'] = test_mask\n","        self.data, self.slices = self.collate([data])\n","    # def _download(self):\n","    #     return\n","    # def _process(self):\n","    #     return\n","    # def __repr__(self):\n","    #     return '{}()'.format(self.__class__.__name__)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"aILk532_BFWh","executionInfo":{"status":"ok","timestamp":1645088062267,"user_tz":-60,"elapsed":12,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["dataset = PPIDATASET()\n","#Here, the dataset contains only a single, undirected citation graph, reminder: dataset is like a dictionary that hold the graph inside, here the dictionary\n","#has only one elemetns:\n","data = dataset[0] #now data is ready for training and testing"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7dsjgy_nDZ1p","executionInfo":{"status":"ok","timestamp":1645088064385,"user_tz":-60,"elapsed":4,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["# Model Definition\n","class GAT(torch.nn.Module):\n","    def __init__(self):\n","        super(GAT, self).__init__()\n","        self.hid = 8     #input channels\n","        self.in_head = 8 #output channels\n","        self.out_head = 1 #output head\n","        #suggestion for later ::::: concatanation:true: for intermadiate layers\n","        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.7)\n","        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False, heads=self.out_head, dropout=0.7)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = F.dropout(x, p=0.7, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.7, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"T-HwFPFqEGsn","executionInfo":{"status":"ok","timestamp":1645088067199,"user_tz":-60,"elapsed":1135,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"outputs":[],"source":["# Seed for reproducible numbers\n","torch.manual_seed(1)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","source":["it is so fast on gpu that it becomes difficult to compare the models. if ran on cpu, it can be seen GAT is slower, for the same graph"],"metadata":{"id":"3L4zItqmDqVY"}},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12389,"status":"ok","timestamp":1645088080148,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"},"user_tz":-60},"id":"L_FgWib3D9En","outputId":"9d3ffc95-66df-4931-8f9e-0853c040be7f"},"outputs":[{"output_type":"stream","name":"stderr","text":["  5%|▌         | 1/20 [00:00<00:05,  3.46it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(2.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 2/20 [00:01<00:14,  1.26it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(8.4588, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 10/20 [00:02<00:01,  6.30it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(9.8140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1351.0387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(85.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 13/20 [00:02<00:00,  9.01it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(22.2186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(22.0784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(27.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(23.6056, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:02<00:00,  6.96it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(34.3958, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Train\n","model = GAT().to(device)\n","data = dataset[0].to(device)\n","\n","# Adam Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=5e-4)\n","\n","# Training Loop\n","model.train()\n","tick=time.time()\n","for epoch in tqdm(range(20)):\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])    \n","    \n","    if epoch%2 == 0:\n","        print(loss)\n","    loss.backward()\n","    optimizer.step()\n","tock=time.time()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn5FhBZdEAaR"},"outputs":[],"source":["\n","model.eval()\n","_, pred = model(data).max(dim=1)\n","correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n","acc = correct / data.test_mask.sum().item()\n","train_acc = metric.accuracy(pred[data.test_mask],data.y[data.test_mask])\n","train_precision=metric.precision(pred[data.test_mask],data.y[data.test_mask],2)\n","train_f1score = metric.f1_score(pred[data.test_mask],data.y[data.test_mask],2)\n","train_recall = metric.recall(pred[data.test_mask],data.y[data.test_mask],2)\n","TP=metric.true_positive(pred[data.test_mask],data.y[data.test_mask],2)\n","TN= metric.true_negative(pred[data.test_mask],data.y[data.test_mask],2)\n","FN= metric.false_negative(pred[data.test_mask],data.y[data.test_mask],2)\n","FP= metric.false_positive(pred[data.test_mask],data.y[data.test_mask],2)\n","print(\"training done in \",(tock-tick),\"seconds\")\n","print('test Accuracy: %s' % train_acc)\n","print('test precision: %s' % list(train_precision)[1].item())\n","print('test f1 score: %s' % list(train_f1score)[1].item())\n","print('test recall: %s' % list(train_recall)[1].item())\n","print(\" #### confusion matrix test: \")\n","print( \"TP\",list(TP)[1].item(),\"FP\",list(FP)[1].item())\n","print( \"TN\",list(TN)[1].item(),\"FN\",list(FN)[1].item())"]},{"cell_type":"markdown","source":["the results for 20 epochs for Asthma (slower than gcn)\n","\n","training done in  111.46260857582092 seconds\n","\n","test Accuracy: 0.18409680207433016\n","\n","test precision: 0.10513019561767578\n","\n","test f1 score: 0.18648740649223328\n","\n","test recall: 0.8246951103210449\n","\n"," #### confusion matrix test: \n","\n","TP 541 FP 4605\n","\n","TN 524 FN 115\n","\n","can we do better with more epochs? ( with 50 epochs )\n","\n","training done in  283.5730781555176 seconds\n","\n","test Accuracy: 0.8866032843560934 accuracy increased\n","\n","test precision: 0.0\n","\n","test f1 score: 0.0\n","\n","test recall: 0.0\n","\n"," #### confusion matrix test: \n","\n","TP 0 FP 0\n","\n","TN 5129 FN 656\n"," "],"metadata":{"id":"wBlv3JrIEgHz"}},{"cell_type":"markdown","source":["# **for balanced version for Asthma**\n","\n","\n","training done in  31.540491819381714 seconds (more or less same time, not better/worst results) >  it doesnt mean it is worst than GCN ! because in GCN results with another initializations can be more or less (accuracy between 0.35 to 0.45) \n","\n","\n","**test Accuracy:** 0.3854462762933485   > lower than gcn\n","\n","**test precision** : 0.3854462802410126\n","\n","**test f1 score**: 0.5564218163490295    > higher than gcn > \n","\n","**test recall**: 1.0\n","\n"," #### confusion matrix test: \n","TP 678 FP 1081\n","\n","TN 0 FN 0\n","\n","so maybe in my opinion i can say in term of metrics this is better than GCN"],"metadata":{"id":"oiZhaaQ-LyPH"}},{"cell_type":"code","source":["# Model Definition\n","class GAT(torch.nn.Module):\n","    def __init__(self):\n","        super(GAT, self).__init__()\n","        self.hid = 8     #input channels\n","        self.in_head = 8 #output channels\n","        self.out_head = 1 #output head\n","        #suggestion for later ::::: concatanation:true: for intermadiate layers\n","        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.7)\n","        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False, heads=self.out_head, dropout=0.7)\n","\n","    def forward(self, x, edge_index, edge_weight):\n","        x, edge_index = data.x, data.edge_index\n","        x = F.dropout(x, p=0.7, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.7, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"TkcOWHy43i3x","executionInfo":{"status":"ok","timestamp":1645088138376,"user_tz":-60,"elapsed":415,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","class Net(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = GCNConv(dataset.num_features, 16, normalize=False)\n","        self.conv2 = GCNConv(16, dataset.num_classes, normalize=False)\n","\n","    def forward(self, x, edge_index, edge_weight):\n","        x = F.relu(self.conv1(x, edge_index, edge_weight))\n","        x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"hvBYnj_F31Me","executionInfo":{"status":"ok","timestamp":1645088141875,"user_tz":-60,"elapsed":543,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = GAT().to(device)\n","data = data.to(device)\n","lr = 1e-1\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","epochs = 200\n","\n","x, edge_index, edge_weight = data.x, data.edge_index, None\n","\n","num_epochs = 200\n","\n","for epoch in tqdm(range(num_epochs)):\n","    model.train()\n","    optimizer.zero_grad()\n","    log_logits = model(x, edge_index, edge_weight)\n","    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","\n","with torch.no_grad():\n","  model.eval()\n","  logits = model(x, edge_index, edge_weight)\n","\n","  #test_mask = data['test_mask']\n","  preds = logits.max(1)[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4VRGtCX4Tlf","executionInfo":{"status":"ok","timestamp":1645088167514,"user_tz":-60,"elapsed":5705,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}},"outputId":"2744ea82-150b-4bfa-abf2-7d8bee7a1af9"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [00:05<00:00, 37.41it/s]\n"]}]},{"cell_type":"code","source":["explainer = GNNExplainer(model, epochs=200, return_type='log_prob', num_hops = None) #if num_hops is none it is detected from the num of message passing ops.\n","                                                                                  #it is needed to tell the explainer \"how far to go\" to look for explanations.\n","node_idx = 11 \n","pred_node_idx = preds[node_idx].item()\n","print(\"Explaining node \", node_idx, \" with predicted class: \", pred_node_idx)\n","node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index, edge_weight=edge_weight)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOKGkSqX4ZI-","executionInfo":{"status":"ok","timestamp":1645088511716,"user_tz":-60,"elapsed":429,"user":{"displayName":"alireza samadi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1rvUsWUWglw4bHdIjOXTa4XhrFtdw1wUpGtsY=s64","userId":"15397229150758724266"}},"outputId":"ec489e2b-1a31-4229-fc06-424f91746b3a"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Explaining node  11  with predicted class:  1\n"]}]},{"cell_type":"code","source":["ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y, seed = node_idx, threshold = None) #you can set threshold to define the hard mask accordign to the sparisty you wnat ot obtain and how much you want to be strict\n","plt.show()"],"metadata":{"id":"nsbqIcYR65UM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"2.GAT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}